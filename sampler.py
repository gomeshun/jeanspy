import pandas as pd
import numpy as np
import emcee 
import emcee.backends
from astropy.io import ascii
from astropy.coordinates import SkyCoord,Distance
import astropy.units as u
from tqdm import tqdm as tqdm
from multiprocessing import Pool

############################################
# # How to use blobs in emcee
# 
# import emcee
# import numpy as np

# def log_prior(params):
#     return -0.5 * np.sum(params**2)

# def log_like(params):
#     return -0.5 * np.sum((params / 0.1)**2)

# def log_prob(params):
#     lp = log_prior(params)
#     if not np.isfinite(lp):
#         return -np.inf, -np.inf, -np.inf
#     ll = log_like(params)
#     if not np.isfinite(ll):
#         return lp, -np.inf, -np.inf
#     return lp + ll, lp, np.mean(params)

# coords = np.random.randn(32, 3)
# nwalkers, ndim = coords.shape

# # Here are the important lines
# dtype = [("log_prior", float), ("mean", float)]
# sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob,
#                                 blobs_dtype=dtype)

# sampler.run_mcmc(coords, 100)

# blobs = sampler.get_blobs()
# log_prior_samps = blobs["log_prior"]
# mean_samps = blobs["mean"]
# print(log_prior_samps.shape)
# print(mean_samps.shape)

# flat_blobs = sampler.get_blobs(flat=True)
# flat_log_prior_samps = flat_blobs["log_prior"]
# flat_mean_samps = flat_blobs["mean"]
# print(flat_log_prior_samps.shape)
# print(flat_mean_samps.shape)

############################################
# # How to use backends in emcee
# import emcee
# import numpy as np

# np.random.seed(42)

# # The definition of the log probability function
# # We'll also use the "blobs" feature to track the "log prior" for each step
# def log_prob(theta):
#     log_prior = -0.5 * np.sum((theta - 1.0) ** 2 / 100.0)
#     log_prob = -0.5 * np.sum(theta**2) + log_prior
#     return log_prob, log_prior


# # Initialize the walkers
# coords = np.random.randn(32, 5)
# nwalkers, ndim = coords.shape

# # Set up the backend
# # Don't forget to clear it in case the file already exists
# filename = "tutorial.h5"
# backend = emcee.backends.HDFBackend(filename)
# backend.reset(nwalkers, ndim)

# # Initialize the sampler
# sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, backend=backend)

# max_n = 100000

# # We'll track how the average autocorrelation time estimate changes
# index = 0
# autocorr = np.empty(max_n)

# # This will be useful to testing convergence
# old_tau = np.inf

# # Now we'll sample for up to max_n steps
# for sample in sampler.sample(coords, iterations=max_n, progress=True):
#     # Only check convergence every 100 steps
#     if sampler.iteration % 100:
#         continue

#     # Compute the autocorrelation time so far
#     # Using tol=0 means that we'll always get an estimate even
#     # if it isn't trustworthy
#     tau = sampler.get_autocorr_time(tol=0)
#     autocorr[index] = np.mean(tau)
#     index += 1

#     # Check convergence
#     converged = np.all(tau * 100 < sampler.iteration)
#     converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)
#     if converged:
#         break
#     old_tau = tau
############################################


class Sampler:
    """ wrapper class for emcee.EnsembleSampler
    """

    def __init__(self,model,p0_generator,nwalkers=None,reset=False,pool=None,**kwargs):
        """ initialize the sampler.
        
        model: a model class
        p0_generator: a function to generate p0
        nwalkers: number of walkers. default is 2*ndim
        **kwargs: keyword arguments for emcee.EnsembleSampler
        """
        self.model = model
        self.ndim = model.ndim
        self.nwalkers = model.ndim * 2 if nwalkers is None else nwalkers
        self.p0_generator = p0_generator
        self.kwargs = kwargs
        blobs_dtype = [ ("lnl",float), *[ (name, float) for name in self.model.prior_names ]]
        print(blobs_dtype)
        
        # define filename as a comination of model name and current time
        # NOTE: replace "+" in the model name
        import time
        filename = self.model.name.replace("+","_") + ".h5"
        self.backend = emcee.backends.HDFBackend(filename)
        if reset:
            self.backend.reset(self.nwalkers, self.ndim)
        
        log_prob = self.model.lnposterior

        self.sampler = emcee.EnsembleSampler(self.nwalkers,
                                             self.ndim,
                                             log_prob_fn=log_prob,
                                             blobs_dtype=blobs_dtype,
                                             backend=self.backend,
                                             pool=pool,
                                             **self.kwargs)


    def run_mcmc(self,iterations,loops,reset=False,**kwargs):
        """ run the sampler.
        p0 is generated by p0_generator.
        save and monitor the chain and lnprob using backends.
        blobs_dtype is obtained by model.

        iterations: number of iterations for each loop
        loops: number of loops
        """
        # Set up the backend
        # Don't forget to clear it in case the file already exists

        # We'll track how the average autocorrelation time estimate changes
        index = 0
        autocorr = np.empty(loops)

        # This will be useful to testing convergence
        old_tau = np.inf

        # Now we'll sample for up to  steps
        for i_loop in range(loops):
            # start mcmc sampling with self.sampler.run_mcmc and pool
            initial_state = self.p0_generator(self.nwalkers) if self.backend.iteration == 0 else None
            self.sampler.run_mcmc(initial_state,iterations,
                                    progress=True,
                                    **kwargs)
            
            # Compute the autocorrelation time so far
            # Using tol=0 means that we'll always get an estimate even
            # if it isn't trustworthy
            tau = self.sampler.get_autocorr_time(tol=0)
            autocorr[index] = np.mean(tau)
            index += 1

            # Check convergence
            converged = np.all(tau * 100 < self.sampler.iteration)
            converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)
            if converged:
                break
            else:
                print(f"tau:{tau}\titeration:{self.sampler.iteration}")
                old_tau = tau

        

import swyft

class DSphSimulatior(swyft.Simulator):
    """ Simulator class for dSph model.
    """
    def __init__(self,R_pc,model):
        super().__init__()
        self.transform_samples = swyft.to_numpy32  # convert samples to numpy array
        self.R_pc = R_pc  # Position of observed stars in pc
        self.model = model  # model class for dSph

    def get_slos(self,p):
            """ get line-of-sight velocity from model.
            """
            params = self.model.convert_params(p)
            self.model.update("all",params)
            slos = self.model.submodels["DSphModel"].sigmalos_dequad(self.R_pc)
            return slos

    def build(self,graph):
        """ build the simulator.
        """
        p = graph.node("p", lambda: self.model.sample())
        # find "vmem_kms" in model.submodels["PriorModel"].data (pandas.Series)
        # and get the index
        vmem_kms_index = self.model.submodels["FlatPriorModel"].data.index.get_loc("vmem_kms")
        vmem_kms = graph.node("vmem_kms", lambda p: p[vmem_kms_index], p)

        slos = graph.node("slos_kms", self.get_slos, p)  # shape: (nstars,)
        v_kms = graph.node("vlos_kms", lambda vmem_kms,slos: vmem_kms + slos * np.random.randn(len(self.R_pc)), vmem_kms, slos)
        